# 第九章 渲染管线与图形学

## Quaternion 四元数

### 为什么不是欧拉角

欧拉角的定义是，x,y,z 分别表达了x轴上的旋转角度，y轴上的旋转角度，z轴上的旋转角度，即(20,40,50) 表达了在x轴上旋转20度，y轴上旋转40度，z轴上旋转50度。看起来简单易懂就能定义坐标系上的旋转角度，为什么就不能使用它来表达所有的旋转角度，却还要使用四元数呢？其实是有原因的。

欧拉角存在别名，100度的旋转角度可以用-260度来表示，370度角与10度角以及-350度角是相同的旋转角度，这种表达方式在计算上特别难统一。欧拉角在插值上存在些问题，一个-260度的角度和一个50度的角度进行插值是需要先进行转换的，先将-260度转换成100度，再进行插值才可以得到正确的结果。简单的别名问题虽然讨厌，但是可以转换角度的方式解决，只是转换角度这种方式并不是一个靠谱的方式，在计算过程中会遇到相当多的麻烦。

欧拉角这种周期性和旋转之间的不独立性造成了欧拉角在线性变化的计算中比较困难，因此我们需要寻找在计算过程中更加便捷的方法，不需要转换，没有别名，统一规格。四元数恰好就满足了这些需求，它在线性变换中能统一且灵活，它有个致命缺陷就是在表现上让人比较难理解，一般人一眼看不出一个四元数所表达的旋转的方向和角度。

### 四元数的由来

四元数可以说是图形数学中的“异次元”，这也是为什么普通人难以理解它的原因，为什么说它是一个“异次元”呢？我们所有的向量，坐标，矩阵，旋转，缩放，平移，都是建立在使用坐标系和空间矩阵计算的这一套计算体系上的，而四元数则不是，它特立独行，它计算所用的公式，并不是建立在传统的坐标系矩阵上，而是拥有自己一套“自有”的公式体系，即复数体系。

四元数有两种记法即：

```
	向量v 加 w分量 (v, w)

	四个分量都分开 (x, y, z, w)
```

某些情况下v分量更方便，而在另一些情况下分开记会更清楚。但其实这个w分量和x,y,z的相关度有但不是很大，因此我们不要被迷惑了，以为他们的值越大或者越小会怎样怎样的，其实和我们肉眼看到的是有所差别的。

最早数学家们是用复数系统来表达二维中的旋转的。我们来回忆下什么是复数？

```
	我们把形如 z=a+bi（a,b均为实数）的数称为复数，其中a称为实部，b称为虚部，i称为虚数单位，其中虚数 i * i = -1。
	
	当z的虚部等于零时，常称z为实数；
	
	当z的虚部不等于零时，实部等于零时，常称z为纯虚数。
```

数学家使用复数对(a, b)来表达二维平面中的旋转，他们把向量也定义为复数对，如果(a,b)为向量的话，那么就定义了 a + b * i，i为虚数，满足 i * i = -1，a为实轴的坐标，b为虚轴坐标，我们可以理解为x，y轴上的坐标。

对于定义一个旋转复数，为 (cos(β), sin(β)) 表达式为 cos(β) + sin(β) * i，i为虚数，β为旋转的角度。他们发现从平面上求得某个向量旋转 β 的结果，可以用向量复数对乘以旋转复数对的方式来获得，即

```
	缺少图片

	v = x + y * i

	r = cos(β) + sin(β) * i

	v' = vr = (x + y * i)(cos(β) + sin(β) * i)

	   = (x * cos(β) - y * sin(β)) + (x * sin(β) + y * cos(β)) * i
```

上述图中的公式推导，乘法的运算跟传统的计算有点差异，因为我们表达式中有复数运算，其中复数i满足 i * i = -1。

这是最早发明的二维向量旋转运算法则，由十六世纪被意大利米兰学者卡当提出复数后，经过达朗贝尔、棣莫弗、欧拉、高斯等人的工作，最后成形的数学体系。不过二维的上的旋转公式很快就不够用，无法对三维的旋转操作，旋转复数局限性太大。爱尔兰数学家 William Hamilton 终于在1843年找到了一种表达三维旋转的复数表达方式，即四元数就此诞生。

### 四元数的几何意义

四元数扩展了复数系统，它使用了三个虚部，即i，j，k，因此四元数的表达复数表达方式为

```
	q = w + i * x + j * y + k * z

	其中i，j，k为虚数，即满足

	i*i = j*j = k*k = -1

	i*j=k, ji=-k

	j*k=i, k*j=-i

	k*i=j, i*k=-j
```

与复数能用来旋转二维中的向量类似，四元数也能用来旋转三维中的向量。

四元数被解释为角位移的轴一角方式。什么是轴一角？就是绕某个单一轴旋转一个角位移就能表达旋转的方式就叫轴一角，这个角位移其实就是一个和向量类似的表达方式，即(x,y,z)，只不过四元组用4个元素来表达罢了。说白了，四元组可以理解为绕 某个轴N 旋转的角位移，和欧拉角用x,y,z表达绕标准坐标轴旋转是同样的道理，只不过这个轴不再是标准轴，而是任意轴。

这也就说明了，四元组不再受到标准轴的限制，它可以表达绕任意轴旋转的角位移。四元组表示为对任意轴 N 的角位移即：

```
	q = [cos(β/2), sin(β/2) * N]

	  = [cos(β/2), sin(β/2) * Nx, sin(β/2) * Ny, sin(β/2) * Nz]
```

利用上图的四元组表达方式，假设我们绕的是某个单轴旋转，也就是在x轴上旋转 A 度，或者在y轴上旋转 B 度，或者在z轴上旋转 C 度，根据上述的公式，我们可以得到三个供旋转的四元数，即：

```
	A' = [cos(-A/2), sin(-A/2) * 1, 0, 0]

	B' = [cos(-B/2), 0, sin(-B/2) * 1, 0]

	C' = [cos(-C/2), 0, 0, sin(-C/2) * 1]
```

上图中的角度为什么是负的呢，因为我们在旋转点时的角度，和在旋转坐标系时的角度恰恰是相反的，因此操作旋转点的角度其实就是旋转坐标系反方向的角度，即负的角度。

现在我们要计算绕x轴上旋转A角度，并且绕y轴上旋转B角度，并且绕z轴上旋转C角度的四元组时，可以把 A‘，B’，C‘，这三个四元组乘起来，即：

```
	A'B'C' = (A'B'C') 最终计算得到

	[x,]
	[y,]
	[z,]
	[w]

	等于

	[cos(B/2)sin(A/2)cos(C/2) + sin(B/2)cos(A/2)sin(C/2),]
	[sin(B/2)cos(A/2)cos(C/2) - cos(B/2)sin(A/2)sin(C/2),]
	[cos(B/2)cos(A/2)sin(C/2) - sin(B/2)sin(A/2)cos(C/2),]
	[cos(B/2)cos(A/2)cos(C/2) + sin(B/2)sin(A/2)sin(C/2)]
```

上图中得到的最后公式就是从欧拉角转换到四元数的计算公式。

四元数的虽然在计算上很方便且通用，但在辨识度上却存在着严重的缺陷，人肉眼很难分辨某个四元数的旋转情况。但是没关系，我们只要理解它的原理，并且已经知道了四元数的几何意义，在运用四元数过程中便能更加自如。



## 渲染管线

### OpenGL、DirectX究竟是什么

OpenGL 和 DirectX 其实是一回事，它们都是图形渲染的应用程序编程接口，它们都是一种可以对图形硬件设备特性进行调用的软件库。它们的区别只是服务的系统可能有所不同，接口的命名方式也有所不同，DirectX专门服务于微软开发出来的系统，比如Windows和Xbox，他们本身分别由不同的两个组织开发出来。

为什么会是两个不同群体开发出来的两套差不多功能的软件，并且还同时运行在现有的世界中呢？OpenGL是由SGI(Silicon Graphics 美国硅图公司)开发的，而DirectX是由微软开发的，由于市场竞争的关系两家公司做了同样的事，最后导致现在的局面。这种局面也很正常，打个比方，你会用微信去聊天也有时会用旺旺去聊天，都是聊天只是聊天的场景不同心情和目标有点不同而已，图形接口也是一样，不只是OpenGL和DirectX还有专门为苹果系统服务的Metal。从现在的局面来看，我们可以想象的到，在当初还没有形成统一的硬件渲染接口时，各家公司的图形编程接口有多混乱情况有多复杂，对标准统一接口的标准竞争有多激烈。在这种严峻的环境下才使得两家公司为了各自的利益，一直在不断维护和升级着各自的驱动接口直到今天。

幸运的是 Unity3D 已经帮我们封装好了 OpenGL 和 DirectX 的接口，我们无需关心到底是调用 OpenGL 还是 DirectX。我们这里会以OpenGL为例来讲解渲染过程，DirectX也是相似的原理与过程。

### OpenGL处在哪个位置

Unity3D通过调用 OpenGL 的图形接口来渲染图像，OpenGL 定义了各种标准接口就是为了让像 Unity3D 这样的应用程序在面对不同类型的显卡硬件时可以不必慌张，也由于 OpenGL 的存在Unity3D完全不需要去关心硬件到底是哪个厂家生产的，以及它们的驱动是什么。与其说OpenGL在标准接口中适配了硬件厂商的驱动程序，不如说硬件厂商的驱动程序适配了OpenGL，事实上确实是这样。

当Unity3D渲染调用时会去设置OpenGL的渲染状态，OpenGL就会去检查显卡驱动程序里是否有该功能，有就会调用没有就不调用，那些比较特殊的渲染功能接口有些底端的硬件上并没有该功能。显卡驱动与OpenGL不同的是，显卡驱动是在硬件之上的专门为硬件服务的程序，它是用来将指令翻译成机器语言并调用硬件的那个程序，而OpenGL则调用的只是显卡驱动。

显然OpenGL是在驱动程序之上的应用程序接口，我们可以把它看做是适配了很多不同驱动程序的中间件。如下图：

![opengl](.\Pictures\9\render-pipeline1.png)

上图指出了OpenGL所处的位置，图形引擎Unity3D调用OpenGL图形接口，告诉OpenGL某个模型数据需要渲染或者说某个渲染状态需要要设置，再由OpenGL发送指令给显卡驱动程序，显卡驱动程序将指令翻译为机器码后，将指令机器码发送到GPU，最后GPU根据指令做相应的处理。我们看到这个过程中显卡驱动程序只是做了传递指令消息的工作，指令从OpenGL那里发起到GPU接收到指令，显卡驱动起到了翻译的工作。

当然这里GPU不只会处理一次，OpenGL会通过显卡驱动发送很多次指令给GPU，让它处理一连串的操作，每次指令都有可能不一样，经过一系列的处理过程后，最终形成了一张屏幕大小的图像存放在缓存中，这时GPU才向屏幕输出最终画面。

下面我们就来详细介绍一下这一整条渲染管线的处理过程。

### 渲染管线是什么？

上面所说的 OpenGL 通过驱动程序向 GPU 发送很多个指令，这一系列指令加起来才形成一整个渲染画面。

渲染管线就是指令中完成一个绘制命令(drawcall)的流水线。这条流水线中有很多个环节，每个环节都自己干自己的事，就像工厂里的流水作业一样，每个节点的工人都会拧属于自己的螺丝，完全不会去管前面节点发生了什么事情。在现代GPU中也会做些流程上的优化，比如某种情况下跳过某些节点的以节省开销，但节点还是自顾自的工作，这部分会在后面的文章中提到。

我们可以描述说渲染管线是一系列数据处理的过程，这个过程最-终的目的是将应用程序的数据经过计算最终输出到帧缓存上最后输出到屏幕。渲染管线从接受到渲染命令后开始，分几个阶段处理了这些数据，这几个阶段分别是应用阶段，几何阶段，光栅化阶段，经过这几个阶段处理最终输出到屏幕上。

#### 应用阶段

应用阶段就是我们执行Unity3D引擎和业务逻辑的过程，在逻辑代码执行过程中，我们实例化了很多个模型或者UI（UGUI的UI也是网格，跟渲染场景中的3d模型从根本上是没有区别的），这些模型有贴图，有材质球，有网格，对于引擎来说，在这个阶段代码执行完毕后它知道了哪些模型需要被渲染，有哪些光源存在，摄像头的位置大小，总的来说这个阶段是准备渲染数据的阶段，为调用渲染准备。

引擎除了知道有哪些东西(数据)需要被提交到GPU渲染外，在提交前还会对这些数据做很多优化工作，从而省去很多不必要的渲染工作提高性能。优化工作有很多，前面的章节中我们也讲到了很多很多关于在逻辑端上优化的工作，这里我们来重点说一下引擎的‘剔除’优化部分。

Unity3D引擎会对不需要渲染的物体进行剔除，原本这是在GPU中做的事搬到了CPU上做。为什么要搬到CPU上做呢？因为在引擎端掌握的是第一手数据信息，引擎大可以从粗颗粒上下手做剔除工作，这样GPU的负担就会减轻很多，倘若所有的网格都放到GPU去剔除，会浪费很多算力同时降低了渲染的功效。

引擎在粗颗粒上是怎么剔除的呢，引擎当然不会像GPU那样去计算每个三角面的有效性，而是去计算比较粗的模型颗粒，即以包围盒形式判断模型是否需要被剔除。

**引擎会计算一个模型的包围盒，包围盒信息存放在Unity3D的Mesh.bounds变量中，这个包围盒是一个长方体我们常称它为AABB包围盒，即一个顶点与长宽高四个变量组成。我们可以理解为包围盒是个立方体有8个顶点，这8个顶点决定了这个模型是否会被剔除，在判断模型是否有被需要渲染的可能时只要有一个顶点在摄像机可视范围内(锥视体内或正交范围)，就不会被剔除，否则将被无情的抛弃。**

引擎通过这种快速的判断包围盒与锥视体的关系来剔除不需要渲染的物体，以达到对模型粗颗粒的渲染优化。除此之外，对粗颗粒的剔除判断还有 occlusion culling 即遮挡剔除，也是属于应用阶段的优化剔除，它其实也是属于业务逻辑层的优化方案并不是所有项目都会使用，而通常只在第一人称视角的游戏上使用这种剔除方式，通过对场景的离线计算来确定每个点所看能看到的物体的列表数据，将这份数据保存下来，当角色在此场景中时根据当前点的位置去寻找已经计算好的可见物体列表既然展示物体，这样就不需要实时去判定物体是否可见，也能轻松得到可见物体的列表，即在离线时在该位置被计算为被遮挡的物体不会进入渲染队列。

应用阶段的最后时刻就是向GPU提交需要渲染的数据，通常数据会被拷贝到显存中，接着设置渲染参数，最后调用渲染接口。在PC端中显存的位置是最接近GPU的内存设备，将数据拷贝到显存中会加速GPU的工作效率，但在移动端里并没有显存，安卓和IOS的架构决定了它们只能用内存来为GPU提供服务，因此在手机端中没有拷贝数据到显存的这个过程，它们使用的都是同一个物理内存地址，除非我们需要读写这块内存的内容才会将它们另外复制一份以条件CPU与GPU之间的协作(三重缓存机制)。

**什么是渲染状态**

很多人都很困惑，其实就是一连串的开关或方法以及方法的地址指向。比如要不要开启混合，使用哪张纹理，使用哪个顶点着色器，使用哪个片元着色器，剔除背面还是剔除前面亦或都不剔除，使用哪些光源等等。通俗的来说，设置渲染状态，就是设置并决定接下来的网格如何渲染，有了渲染的具体方法，至于具体的渲染工作则是由GPU来执行。

有了渲染的具体方法，就要调用渲染的具体对象，这就是渲染调用做的工作。实际上 Draw call 就是一个渲染指令，发起方是CPU接收方是GPU，这个指令仅仅指向了一连串的图元（即点,线,面，我们可以理解为网格被拆分后的状态），并不会包含其他任何材质信息。每个 Draw call 前面都伴随着一连串渲染状态的设置，因此整个渲染命令队列中都是渲染指令与渲染状态交替出现。

为什么要有渲染命令队列呢？因为CPU和GPU相当于是两个大脑，它们是分离的就像两个线程那样如果没有很好的协调机制，它们无法正常梳理自己的工作。命令缓冲队列就是用来协调CPU与GPU的，CPU向命令缓冲队列中推入指令，GPU则从中取得指令并处理。这个命令缓冲队列成了CPU与GPU的关系纽带，这条关系纽带(命令缓冲队列)很好的连接了CPU与GPU，但也成了它们之间交互的瓶颈，即我们通常所说的 Draw call 太多时GPU的工作效率比较差。其根本原因就是 CPU 发送了渲染指令后，只是空转的等待GPU完成渲染操作。

#### 几何阶段

在几何阶段前引擎已经准备好了要渲染的数据，同时向GPU发送了渲染状态的设置命令和渲染调用命令，接下来的工作就完全属于GPU了。首先进入的是几何阶段的工作，几何阶段的工作目标是将需要绘制的图元(三角形、点、线、面)转化到屏幕空间中，因此它也决定了哪些图元可以绘制以及怎么绘制。图元即点、线、面，我们可以理解为网格的拆分状态，是着色器中的基础数据，在几何阶段作用最大。

**几何阶段会经过几个处理过程，按顺序排列为，顶点着色器、曲面细分着色器、细分计算着色器、几何着色器、图元装配、裁减。**

图中几何阶段分拆成了，顶点着色器、曲面细分着色器、细分计算着色器、几何着色器、图元装配、裁剪6个节点。其中顶点着色器会对每个顶点进行逐一的计算，OpenGL会调用我们在渲染状态设置时的顶点处理函数来处理顶点数据。

顶点处理函数就是我们可编程的部分，它可以很简单只将数据传递到下一个节点，也可以用变换矩阵的方式来计算顶点在投影空间的位置，或者通过光照公式计算来得到顶点的颜色，或者计算并准备其他下一个阶段需要的信息。

曲面细分着色器、细分计算着色器、几何着色器，这三个着色器是非必须着色器，很多手机设备上的GPU并没有这几个功能。细分着色器包括曲面细分着色器和细分计算着色器会使用面片来描述一个物体的形状，并且增加顶点和面片数量使得模型外观更加平顺。几何着色器则允许自定义增加和创建新的图元，这是唯一一个能自定义增加新图元的着色器。

前面几个着色器节点处理的都是顶点数据，到了图元装配节点，它将这些顶点与相关的几何图元之间组织起来为下一步的裁剪工作做准备。

**经历过前面几个阶段的变换，特别是在顶点着色器中将顶点从模型空间转换到投影空间，这个转换过程为从模型空间到世界空间再到视口空间再到投影空间，Unity3D的Shader中常见的UNITY_MVP宏定义变量就是坐标空间转换的变化矩阵，它在渲染前就已经计算好并存储起来不需要再次计算。在转换了坐标空间之后会经硬件上的透视除法得到归一化的设备坐标，归一化的设备坐标会使裁剪更加容易，不仅如此还对后面的深度缓冲和测试有很大的帮助。**

其中归一化后的设备坐标(Normalized Device Coordinates, NDC)可以看做是一个矩形内的坐标体系，这个经转化后的坐标体系是个限制在立方体内的坐标体系，所有在这个坐标体系内的顶点的坐标都不会超过1到-1之间，无论x、y、z轴。

归一化坐标让坐标范围固定在1到-1之间，使得后续对图元数据的处理变得更为简单。不过归一化坐标范围在OpenGL和DirectX上标记也有所不同，在OpenGL上x、y、z坐标范围在[-1,1]之间，而在DirectX上则是[0,1]之间，但这并不影响最终在屏幕上的表达，只是规则不同而已。最终他们都会进行简单的线性变换映射到屏幕的平面矩形范围内。在屏幕映射时，OpenGL和DirectX两者的也有所差异，OpenGL以左下角为(0,0)点，而DirectX则以左上角为(0,0)点，显然这不统一的做法是由于两个商家的竞争而故意造成的，不管怎样差异已经存在了我们只能小心留意。

说了这么多就是为了更好的理解几何阶段最后一步裁剪。我们将顶点转化到了归一化的坐标空间后，裁剪就容易多了。通过图元装配，有了线段和三角形数据，裁剪就可以开始了。

一个三角是否需要被裁剪，由归一化后的坐标判断，它或许完全在范围内，或许完全在范围外，或许部分在里面部分在外面。倘若三角形完全在可视的长方体范围内的则数据会被继续传递下去，完全在范围外的则被剔除掉不再进入到后面的阶段，若是部分在视野内则需要进一步做切割处理，把在范围外的部分剔除掉并在边界处生成新的顶点来连接没有被剔除的顶点。

至此所有几何阶段的操作都结束了，总体来说几何阶段处理的是顶点，以及计算和准备下一个阶段需要用到的数据。

#### 光栅化阶段

光栅化阶段分为三个节点，光栅化、**片元着色器**、逐片元操作。其中光栅化又可以分成，三角形设置、三角形遍历两个节点。

三角形设置即Triangle Setup。前面阶段都是空间意义上的顶点和三角形，到了光栅化阶段我们更加需要的是屏幕上的像素，于是三角形设置可以认为是将所有三角形都铺在屏幕坐标平面上，这样就知道了三角形面片在屏幕上的情况，三角形面片会以三条线的边界形式来表达面片的覆盖面积。其实覆盖面积面积并不是关键，因为屏幕中展示的画面都是以像素为单位计算的，因此一个三角形覆盖哪些像素需要依靠扫描变换(Scan Conversion)得到，这个像素扫描阶段就是三角形遍历(Triangle Traversal)的环节。如下

**经过光栅化过程，我们得到了三角形覆盖的像素上的信息，我们称这些像素为片元，每个片元包含了经过三个顶点中的信息插值后的信息。接着这个片元被传递到下一个阶段即片元着色器。**

片元着色器(Fragment Shader)就如字面意思那样，是处理片元的地方，就是我们上面介绍的光栅化后三角覆盖的像素。在现代的渲染管线上它已经被设计成可编程的阶段，我们在这里可以编写很多技巧来改变片元的颜色，或者也可以直接丢弃该片元(discard 或者 clip)。

每个片元就相当于一个像素，只是比起像素，片元装载了很多的信息，这些信息都是通过前面三角形遍历时对三个顶点中的信息插值得到的。经过片元着色器的处理，也就是我们编写的片元着色程序的处理后，最终输出的也是片元，我们通常都在片元着色器中计算改变片元的颜色，最终得到一个我们想要的输出到屏幕的片元。

**这里有一个重点，每次片元着色器处理片元时都只是单个片元(只是处理片元的单元有很多很多个，他们相互独立)，对于片元着色器来说它并不知道相邻的片元是什么样，因此每个片元在处理时无法得到邻近的片元的信息。**

得不到邻近的片元信息并不代表我们就没有方法让片元受到邻近的片元影响，虽然每次片元着色器传入的和处理的都是单个片元，但GPU在跑片元着色器时并不是只跑一个片元着色器，而是将其组织成2x2的一组片元块同时跑4个片元着色器。

上图中，描绘了4个片元组成的片元组，以及偏导数函数对它们的计算过程。我们可以通过ddx和ddy这两个偏导数函数来求得邻近片元的差值。偏导数函数可以用于片元着色器中的任何变量。对于向量和矩阵类型的变量，该函数会计算变量的每一个元素的偏导数。偏导数函数是纹理Mipmaps实现的基础，我们将在后面的章节中详细讲解。

除了计算片元的颜色，我们还可以在片元着色器中丢弃某些片元(discard 或者 clip)，我们常说的Alpha Test就是一个用丢弃片元函数来实现的效果，我们将在后面的章节中详细讲解 Alpha Test的原理与利弊。

**片元着色器和顶点着色器是我们在着色器编程时最重要的两个节点，如果我们想要更通俗简单的理解顶点着色器和片元着色器的区别的话，可以认为：顶点着色器(包括细分着色器和几何着色器)决定了一个三角形应该放在屏幕的什么位置，而片元着色器则用这个三角形面片计算三角形范围内的像素拥有什么样的颜色。**

片元着色器输出片元后，进入了逐片元操作阶段，也是渲染管线的最后一步。

#### 逐片元操作

逐片元操作(Per-Fragment Operations)是OpenGL的说法，在DirectX称为输出合并阶段(Output-Merger)，其实只是说法不同而已包含的内容都是相同的，它包括了，剪切测试(Scissor test)、多重采样的片元操作、模板测试(Stencil Test)、深度测试(Depth Test)、混合(blending)、以及最后还有个逻辑操作。

这几个节点都是以片元为基础的元素操作，它们大都决定了片元的去留问题。所以逐片元操作阶段是决定片元的可见性问题的一个重要阶段，如果片元在这几个节点上任意一个节点没有通过测试，管线就会停止并丢弃它，之后的测试或操作都不会被执行，反之执行测试全部通过就会进入帧缓存等待输出到屏幕。每个节点实际测试的过程是个相对复杂的过程，而且不同的图形接口实现的细节也不一样，但我们要理解到它们的基本原理和大致的过程则相对简单一些。

所有这些测试和操作其实都可以看做是以开关形式存在，因为他们的操作命令大都包含了On和Off操作指令，在OpenGL里以glEnable()和glDisable()来表示功能是否被开启或关闭，只是除了开关还需要我们指定参数。

**第一步可见性测试就是剪切测试(Scissor)**

它主要针对的是片元是否在矩形范围内的测试判断，如果片元不在矩形范围内则被丢弃。这个范围是一个矩形的区域，我们可以通过OpenGL的函数调用来设置矩形位置和大小，我们称它为剪切盒。实际上我们可以设置很多个剪切盒，只是默认情况下所有渲染测试都在第一个剪切盒上完成，要访问其他剪切盒就需要几何着色器。剪切测试在Unity3D并不常用，它并不是视口设置不同，后者不会限制屏幕的清理操作。

**第二步是多重采样的片元操作。**

普通的采样只采一个样本或者可以说一个像素，而多重采样则是分散取得多个样本，这些样本可能是附近的几个位置也可能是通过其他算法得到的。因此在多重采样中，采样的片元都有各自的颜色、深度值、纹理坐标，而不是只有一种（具体有多少个取决于子像素的样本数目）。这里的多重采样操作有所不同，它是我们可以自定义的操作模式，自定义部分为alpha影响的采样覆盖率，以及我们可以设置掩码与采样的片元进行‘与’操作。默认情况下，多重采样在计算片元的覆盖率时不会考虑alpha的影响。

**第三步模板测试(Stencil Test)**

模板测试说白了和比大小无异，只是在模板测试中比的方式和比的数字我们可以自定义设置。

在模板测试中模板缓存块是必要的内存块，它与屏幕缓冲大小一致，每个片元在测试时都会先取得自己位置上的模版缓冲位置并与之比较，在通过测试后才被写入到模板缓存中，在整个渲染帧结束前它是不会被重置的，也就是说所有模板测试共享一个模板缓存块。

在模板测试中，开发者通常需要指定一个引用参考值(Reference value)，这个参考值为当前物体的片元的提供了标准参考，然后这个参考值会与模板缓存(Stencil Buffer)中当前片元位置的模板值进行比较，模板缓存中的值是被前面的物体的片元通过测试时写入的值，比较两个值后根据比较的结果做判断是否抛弃片元，判断依据可以是大于、等于、小于或其他等，一旦判断失败片元将被抛弃反之则继续向下传递，另外即使判断成功后我们也可以对其值做其他操作，这类操作可以是替换旧的片元、或增加一定的参考值后再替换、或参考值置零等等。

我们来看看到底有多少种判断和多少种对模板缓存的操作。

```
	Greater	大于模板缓存时判断通过

	GEqual	大于等于模板缓存时判断通过

	Less	小于模板缓存时判断通过

	LEqual	小于等于模板缓存时判断通过

	Equal	等于模板缓存时判断通过

	NotEqual	不等于模板缓存时判断通过

	Always	总是通过

	Never	总是不通过
```

上述这些是对于是否通过测试的判断种类，看上去像是各种比大小的方式。其在Unity3D的Shader中的完整的模板测试写法如下：

```
	Stencil {
	    Ref 2 //指定的引用参考值
	    Comp Equal //比较操作
	    ReadMask 255 //读取模板缓存时的掩码
	    WriteMask 255 //写入模板缓存是的掩码
	    Pass Keep //通过后对模板缓存的操作
	    ZFail IncrSat //如果深度测试失败时对模板缓存的操作
	}
```

从上述看模板测试的步骤简单明了，当前值2与模板缓冲比较，2这个数值可以是从业务层传递进Shader内的参数，倘若2这个数值与模板缓存中的值比较后通过，则做Keep操作保留当前模板缓存中的值，倘若数值2并没通过测试，则片元被抛弃，并且在模板缓冲位置加上2这个数值。读取与写入掩码，意思就是读取模板缓存和写入模板缓存时都要与相应的数值进行与操作，这里的255就是16进制的0xFF，相当于2进制的8个1。

Unity3D模板命令中 Pass 的操作是对通过测试后的参考值与模板缓存做操作，它有如下几种方式可选：

```
	Keep	不做任何改变，保留当前缓存中的参考值
	Zero	当前Buffer中置零
	Replace	将当前的参考值写入缓存中
	IncrSat	增加当前的参考值到缓存中，最大为255
	DecrSat	减少当前的参考值到缓存中，最小为0
	Invert	翻转当前缓存中的值
	IncrWrap	增加当前的参考值到缓存中，如果到最值255时则变为0
	DecrWrap	减少当前的参考值到缓存中，如果到最小为0时则变为255
```

不同的物体可以自己有不同的引用值，我们可以通过Unity3D设置材质球参数的接口将数值传递进入着色器。比较操作也可以多种多样，包括掩码值、成功后的操作动作。这让本来看上去一个简简单单的比大小行为赋予了更多的花样。不止如此，模板缓存里的值除了比较和通过测试后的操作指令外，深度测试也可以影响模板缓存中的值。

**第四步深度测试**。

深度测试主要作用是根据深度判断和覆盖在帧缓冲中的片元。片元中有深度信息，它的来源就是在归一化坐标后三角形顶点Z轴上的值，三角形经过光栅化后，三角内的片元的深度是三个顶点的z坐标的插值，深度测试依靠这个深度来判定是否需要覆盖已经写进帧缓冲里的片元(可别忘了我们说片元就是带着诸多信息的像素)。

我们说深度测试是为了判定最后片元是否写入帧缓存，那么在判定过程中深度测试自己也有自己的缓存，即深度缓存，它可读可写就是为片元深度信息判定而存在的。深度测试工作分为两块，其中一块是片元与缓存的比较即ZTest，另一块是片元信息写入缓存即ZWrite。所有片元只有在ZTest中与缓存中的数据比较并被判定通过的才有ZWrite写入深度信息的资格。

当然我们也可以把ZWrite写入权限关闭，这样渲染物体的所有片元都只能做比较操作判定是否覆盖前面的片元而无法写入深度信息，这也同时导致了它的片元深度无法与其他物体比较。这种写法在半透明物体中很常见，其他时候大部分都是默认开启深度值写入即ZWrite On。

深度测试是怎么比较的呢？还记得前面介绍的模板测试么，重点就是“比大小”，这次比的是片元上的深度与深度缓存中的深度信息，判定通过的就有权利写入深度缓存，深度测试的方法和模板测试的流程和方法简直就是一个妈生出的俩个孩子。深度测试的 ZTest 对应模板测试的 Comp指令，深度测试的 ZWrite 对应模板测试的 Pass指令，先拿当前片元比较缓存中的值再操作缓存，两者简直一模一样。我们来看看例子：

```
	Pass
	{
		ZTest LEqual
		ZWrite On
	}
```

这组深度测试参数表明了，深度判定规则为当物体的这个像素的Z值小于当前深度缓存中相同位置的深度时通过ZTest，通过ZTest后将该像素深度信息写入深度缓存。其中ZTest的可选参数为如下：

```
	Less 小于深度缓存中的值
	LEqual 小于等于深度缓存中的值
	Greater 大于深度缓存中的值
	GEqual 大于等于深度缓存中的值
	Equal 与深度缓存中的值相等
	NotEqual 与深度缓存中的值不相等
	Always 永远通过ZTest
	Never 永远不通过ZTest
	Off 等同于 ZTest Always
	On 等同于ZTest LEqual
```

深度测试与模板测试一样有自己的缓存块，不同的是深度测使用的是像素深度值而不是固定某个值，写入缓存的操作没有模板测试那么多花样。

那么什么是深度值？这个深度值是从哪来的呢？还记得前面顶点着色器中介绍的，顶点在变化坐标空间后z轴被翻转成为了视口朝外的轴。摄像机裁剪空间从锥视体变成了长方体，x、y轴则成为了视口平面上的平面轴，z轴上的坐标成了顶点前后关系的深度值。我们再来看看这幅图片：

正是因为空间坐标的转换和最后的的归一化，使得所有顶点的坐标都在一个长方体空间内，长方体的大小被限制在了-1到1的大小，于是x、y成为了屏幕相对参考坐标，而z则成为了前后关系的深度参考值。我们说的这些只是顶点上的坐标变化，在后面的步骤中三角面被光栅化，每个像素加入了更多信息形成了片元，三角形顶点信息在插值后z坐标也进入了片元数据中，于是z值成为了片元在深度测试中判断的依据。

**第五步混合阶段**

混合阶段实质上并没有丢弃任何片元，但却可以让片元消失不见。

如果一个片元通过了上面所有的测试，那么它就有资格与当前帧缓存中的内容进行混合了。最简单的混合方式就是直接覆盖已有的颜色缓冲中的值，实际上这样不算混合，只是覆盖而已，我们需要两个片元的真正混合。

那么什么叫混合？为什么要混合？混合是两个像素对颜色和alpha值的计算过程，其中一个像素来自要渲染的物体，另一个像素则来自已经写入的帧缓存。我们可以自己指定的操作来制定混合公式，通过配置的公式运算我们能得到想要的效果。

由于渲染物体的像素都是一个接一个的被写入缓冲中，当前物体网格被光栅化成为片元后要写入缓存时，前面已经渲染的物体的片元会被直接覆盖掉，这样的话前后两个片元就没有了任何关联性的操作，开启混合则可以对前后两个片元在颜色上做更多的操作，这可能是我们所期望的。

大多数情况混合与像素的 alpha 值有关，也可以只与颜色有关，混合阶段用的最多的是半透明材质。alpha 是颜色的第四个分量，OpenGL中片元的颜色都会带有 alpha 无论你是否需要它，无论是否你显性地设置了它，alpha默认为1不透明，我们可以用它实现各种半透明物体的模拟就像有色玻璃那样。

说白了混合就是当前物体的片元与前面渲染过的物体的片元之间的操作，那么混合具体有哪些操作呢？我们来看下Unity3D中的混合指令：

```
	Blend SrcFactor DstFactor

	Blend SrcFactor DstFactor, SrcFactorA DstFactorA
```

这里有两种操作方式，第一种是混合时颜色包括了alpha，第二种是将颜色和Alpha分开混合。其中 SrcFactor 这个因子(变量)会与刚刚通过测试的物体片元(即当前物体片元)上的颜色相乘，DstFactor 这个因子(变量)则会取已经在帧缓存中的片元(即缓存中的像素)的颜色相乘，类似的 SrcFactorA 这个因子(变量)会与刚刚通过测试的物体片元(即当前物体片元)上的 alpha 相乘，而 DstFactorA 这个因子(变量)会取帧缓存中的像素并与之 alpha相乘。

**这个过程有两个步骤，第一步是相乘操作，第二步是相乘后的两个结果再相加(还可以选择其他操作模式)。我们称为混合方程**

```
	即 Src * SrcFactor + Dst * DstFactor

	或 SrcColor * SrcFactor + DstColor * DstFactor, SrcAlpha * SrcFactorA + DstAlpha * DstFactorA
```

通常情况下相乘的结果再相加得到最终的混合片元。我们也可以改变这种方程式，让两个结果相减、或者调换位置后相减、取得最大值函数、取得最小值函数，来代替源数据与目标数据之间的操作符。即我们可以选择以下操作符：

```
	BlendOp Add 加法
	BlendOp Sub 减法
	BlendOp RevSub 置换后相减
	BlendOp Min 最小值
	BlendOp Max 最大值
```

上述5种操作符的修改就分别代表了因子相乘后相加、或相减、或置换后相减、或取得最小值、或取得最大值。我们这里拿Sub，Max来举个例子：

```
	当写入BlendOp Sub时，方程式就变成了：

	Src * SrcFactor - Dst * DstFactor

	当写入BlendOp Max时，方程式就变成了：

	Max(Src * SrcFactor, Dst * DstFactor)
```

除了操作符可以变化外，SrcFactor、DstFactor、SrcFactorA、DstFactorA 这四个变量的可以选择为：

```
	One 	代表1，就相当于完整的一个数据
	Zero	代表0，就相当于抹去了整个数据
	SrcColor	代表当前刚通过测试的片元上的颜色(即当前物体片元)，相当于乘以当前物体片元的颜色
	SrcAlpha	代表当前刚通过测试的片元上的alpha(即当前物体片元)，相当于乘以当前物体片元的alpha
	DstColor	代表已经在缓存中的颜色，相当于乘以当前缓存颜色
	DstAlpha	代表已经在缓存中的alpha，相当于乘以当前缓存alhpa
	OneMinusSrcColor	代表缓存上的片元做了 1 - SrcColor 的操作，再相乘
	OneMinusSrcAlpha	代表缓存上的片元做了 1 - SrcAlpha 的操作，再相乘
	OneMinusDstColor	代表当前刚通过测试的片元上的颜色做了 1 - DstColor 的操作，再相乘
	OneMinusDstAlpha	代表当前刚通过测试的片元上的颜色做了 1 - DstAlpha 的操作，再相乘
```

通过操作符号的选择，以及变量因子的选择，我们可以在Blend混合中玩出很多花样来。

SrcFactor、DstFactor、SrcFactorA、DstFactorA 这4个变量因子的选择和操作符的选择决定了混合后的效果，我们来看看常用的混合方法和效果：

**1， 透明度混合Blend SrcAlpha OneMinusSrcAlpha，即常用半透明物体的混合方式。**

这是最常用的半透明混合，首先要保证半透明绘制的顺序比实体的要后面，所以Queue标签是必要的Tags {“Queue” = “Transparent”}。Queue标签告诉着色器此物体是透半透明物体排序。至于渲染排序Queue的前因后果将在后面的文章介绍。

![blend](.\Pictures\9\shader36.png)

Blend SrcAlpha OneMinusSrcAlpha 我们来解释下，以上图为例，图中油桶是带此Shader的混合目标。

当绘制油桶时，后面的实体BOX已经绘制好并且放入屏幕里了，所以ScrAlpha与油桶渲染完的图像相乘，部分区域Alpha为0即相乘后为无(颜色)，这时正好另一部分由OneMinusSrcAlpha(也就是1-ScrAlpha)为1即相乘后原色不变，两个颜色相加后就相当于油桶的透明部分叠加后面实体Box的画面，于是就形成了上面的这幅画面。

反过来也是一样，当ScrAlpha为1时，源图像为不透明状态，则两个颜色在相加前最终变成了，源图像颜色+无颜色=源图像颜色，于是就有了上图中油桶覆盖实体Box的图像部分。

**2，加白加亮叠加混合 Blend One One，即在原有的颜色上叠加屏幕颜色更加白或亮。**

![blend](.\Pictures\9\shader37.png)

第一参数One代表本物体的颜色。第二个参数代表缓存上的颜色。两种颜色没有任何改变并相加，导致形成的图像更加亮白。这样我们就看到了一个图像加亮加白的图像。

**3，保留原图色彩Blend One Zero，即只显示自身的图像色彩不加任何其他效果。**

![blend](.\Pictures\9\shader38.png)

本物体颜色，加上，零，就是本物体颜色。

**4，自我叠加（加深）混合Blend SrcColor Zero，即源图像与源图像自我叠加。**

![blend](.\Pictures\9\shader39.png)

与上面相比，加深了本物体的颜色。先是本物体的颜色与本物体的颜色相乘，加深了颜色，第二个参数为零，使得缓冲中的颜色不被使用。所以形成的图像为颜色加色的图像。

**5，目标源叠加（正片叠底）混合Blend DstColor SrcColor，即把目标图像和源图像叠加显示。**

![blend](.\Pictures\9\shader40.png)

第一个参数，本物体颜色与缓存颜色相乘，颜色叠加。第二个参数，缓存颜色与本问题颜色相乘，颜色叠加。两种颜色相加，加亮加白。这个混合效果就如同两张图像颜色叠加后的效果。

**6，软叠加混合Blend DstColor Zero，即把刚测试通过的图像与缓存中的图像叠加。**

![blend](.\Pictures\9\shader41.png)

与前面的叠加混合效果相似，这个只做一次叠加，并不做颜色相加操作，使得图像看起来在叠加部分并没有那么亮白的突出。因为第二个参数为零，表示后面的屏幕颜色与零相乘即为零。

**7，差值混合BlendOp Sub，Blend One One，即注重黑白通道的差值。**

![blend](.\Pictures\9\shader42.png)

在这个混合中使用了混合操作改变，从默认的加法改成了减法，使得两个颜色从加法变为了减法，不再是变白变亮的操作，而是反其道成为了色差的操作。

除了对源片元和目标片元，相乘再相加的操作，还可以改变相乘后的加法操作。比如减法，取最大值，取最小值等。

Blend混合如同 Photoshop 中对图层操作，Photoshop中每个图层都可以选择混合模式，混合模式决定了该层与下层图层的混合结果，而我们看到的都是是混合后的图片。

#### 逻辑操作

在像素混合结束后，片元将被写入缓存中去，在写入缓冲前还会做一次逻辑操作，这是片元的最后一个操作。它作用于当前刚通过测试的片元和当前帧缓存中的数据之间，逻辑操作会在它们之间进行一次操作，最后再写入到帧缓存。

由于这个过程的实现代价对于硬件来说是非常低廉的，因此很多系统都会允许这种做法。逻辑操作不再有因子，只在两个像素之间操作，操作可以选择为，异或(XOR)操作，与(AND)操作，或(OR)等。只是由于它使用的比较少，也可以由其他方式代替，因此Unity3D中并没有自定义设置逻辑操作的功能。

#### 双缓冲机制

片元最后都会以像素的形式写入帧缓冲中，帧缓冲一边由GPU不断写入，一边由显示器不断输出，这会导致画面还没形成前就绘制到屏幕的情况，所以GPU通常采用双缓冲机制，即前置缓存用于呈现画面，后置缓存则继续由GPU不断写入，写入所有像素后再置换两个缓存，置换时只要置换指针地址就可以了。

当整个画面绘制完成时，后置缓冲与前置缓冲进行调换，于是后置缓存成为了前置缓冲并呈现到屏幕上，原来的前置缓存成为了后置缓冲交由GPU作为帧缓冲继续绘制下一帧，这样就可以保证显示与绘制不会相互干扰。

**整个渲染管线已经全部呈现在这里了，我们来总结一下。**

整个渲染管线从大体上分，应用阶段，几何阶段，光栅化阶段。

从应用阶段开始，数据在应用阶段被记录、筛选(或者也可以叫裁剪)、合并。在筛选、合并中，有些运用了算法来达到裁剪的目的，有些放大了颗粒度以使得加速筛选(裁剪)，有些则利用了GPU工作原理合并了渲染数据提高了GPU工作效率。

几何阶段则着重于处理顶点的数据，顶点着色器是其中最为重要的一个着色器，它不但需要计算顶点在空间上的转换，还要为下一个阶段光栅化阶段做了充分的准备。在顶点着色器中，计算和记录了片元着色器需要的数据，这些数据都会被放入顶点(图元)数据中，顶点上数据会在下一个阶段经过插值计算后放入片元中，每个像素上的数据都是三角形顶点上的数据做插值后所得。

光栅化阶段主要任务是将三角形面转化为实实在在的像素，并且根据顶点上的数据做插值得到片元信息，一个片元就相当于一个像素附带了很多插值过的顶点信息。片元着色器在光栅化阶段起了重要的作用，它为我们提供了可自定义计算片元颜色的可编程节点，不但如此，我们还可以根据自己的喜好抛弃(discard)某些片元。

片元在片元着色器计算后还需要经过好几道测试才能最终呈现在画面上，包括判断片元前后覆盖的深度测试，可以自定义条件的模板测试，以及常用来做半透明的像素混合操作，片元只有经过这几道’关卡‘才能最终被写入帧缓存中，双缓冲机制使得GPU可以尽情的写缓存而无需关心是否会因为写了一半被呈现到画面的问题。

关于渲染管线我们讲解了很多，但还是有很多很多细节被忽略，我们会在后面的章节中详细为大家解剖，这些细节可能会根据各个图形编程接口(OpenGL和DirectX)背后的实现原理而来，也可能是某种渲染方法和技巧其背后的原理。

